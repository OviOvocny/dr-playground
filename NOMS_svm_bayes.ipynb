{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic data and Feature preparation\n",
    "- Load parquet data \n",
    "- Select features\n",
    "\n",
    "And basicly, prepare pandas dataframe for manipulation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## About SVM and Results\n",
    "- Since the data is not linearly separable, we use SVM with kernel trick\n",
    "- We use our own implementation of GridSearch to find optimal parameters. \n",
    "\n",
    "1. Take smaller batch of data, like 1-2% of the whole data and tune parameters\n",
    "2. With tuned parameters than train the main classifier. It can take a looong time. \n",
    "   \n",
    "SVM training time is not linear, it is quadratic. So, if you double the data, it will take 4 times longer to train and it really can be run on gpu.\n",
    "So take a deep breath, beefed up computer and run it. (hours)\n",
    "- for 10% data ~ 20min\n",
    "- for 50% data ~ 2h\n",
    "- for 20% data ~ 1h\n",
    "- for 100% data ~ 10h\n",
    "\n",
    "\n",
    "### Results\n",
    "The more data you use, the better results you get.\n",
    "For 1% of data, we got around ~0.6 f1 score\n",
    "For 20% of the data, we got around ~0.79 f1 score\n",
    "\n",
    "\n",
    "I expect around 0.9-0.96 f1 score for 100% of the data.... But it needs to be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import time\n",
    "\n",
    "phishing = pq.read_table('./floor/phishing_2307.parquet')\n",
    "benign = pq.read_table('./floor/benign_2307.parquet')\n",
    "\n",
    "from transformers.drop_nontrain import drop_nontrain_table as drop_nontrain\n",
    "phishing = drop_nontrain(phishing)\n",
    "benign = drop_nontrain(benign)\n",
    "\n",
    "# realign schemas (parquet files save in nonsense orders)\n",
    "benign = benign.cast(phishing.schema)\n",
    "\n",
    "# concatentate tables\n",
    "data = pa.concat_tables([phishing, benign])\n",
    "df = data.to_pandas()\n",
    "\n",
    "from transformers.cast_timestamp import cast_timestamp\n",
    "df = cast_timestamp(df)\n",
    "\n",
    "used_features = [\n",
    "    \n",
    "    # IP  ===============================\n",
    "    # old (Adam) & still used\n",
    "    \"ip_mean_average_rtt\",\n",
    "    \"ip_entropy\",\n",
    "    \n",
    "    # new\n",
    "    \"ip_count\", \"ip_v4_count\", \"ip_v6_count\",\n",
    "    \n",
    "    \n",
    "    # DNS  ===============================\n",
    "    # old (Adam) & still used\n",
    "    \"dns_A_count\",\n",
    "    \"dns_AAAA_count\",\n",
    "    \"dns_CNAME_count\",\n",
    "    \"dns_MX_count\",\n",
    "    \"dns_NS_count\",\n",
    "    \"dns_SOA_count\",\n",
    "    \"dns_TXT_count\",\n",
    "    \"dns_soa_primary_ns_len\",\n",
    "    \"dns_soa_primary_ns_level\", # renamed\n",
    "    \"dns_soa_primary_ns_digit_count\",\n",
    "    \"dns_soa_primary_ns_entropy\",\n",
    "    \"dns_soa_email_len\",\n",
    "    \"dns_soa_email_level\", # renamed\n",
    "    \"dns_soa_email_digit_count\",\n",
    "    \"dns_soa_email_entropy\",\n",
    "    \"dns_soa_serial\",\n",
    "    \"dns_soa_refresh\",\n",
    "    \"dns_soa_retry\",\n",
    "    \"dns_soa_expire\",\n",
    "    #\"dns_soa_neg_resp_caching_ttl\",\n",
    "    \"dns_mx_mean_len\",\n",
    "    \"dns_mx_mean_entropy\",\n",
    "    \"dns_domain_name_in_mx\",\n",
    "    #\"dns_txt_google_verified\",\n",
    "    \"dns_txt_spf_exists\",\n",
    "    \"dns_txt_mean_entropy\",\n",
    "    \n",
    "    # new\n",
    "    \"dns_txt_dkim_exists\",\n",
    "    \n",
    "    # TLS  ===============================\n",
    "    # old (Adam) & still used\n",
    "    \n",
    "    \"tls_broken_chain\",\n",
    "    \"tls_expired_chain\",\n",
    "    \"tls_total_extension_count\",\n",
    "    \"tls_critical_extensions\",\n",
    "    \"tls_with_policies_crt_count\",\n",
    "    \"tls_percentage_crt_with_policies\",\n",
    "    \"tls_x509_anypolicy_crt_count\",\n",
    "    \"tls_iso_policy_crt_count\",\n",
    "    \"tls_joint_isoitu_policy_crt_count\",\n",
    "    \"tls_iso_policy_oid\",\n",
    "    \"tls_isoitu_policy_oid\",\n",
    "    #\"tls_unknown_policy_crt_count\", <-- Abandoned, no useful values\n",
    "    \"tls_subject_count\",\n",
    "    \"tls_server_auth_crt_count\",\n",
    "    \"tls_client_auth_crt_count\",\n",
    "    \"tls_CA_certs_in_chain_ratio\",\n",
    "    \"tls_unique_SLD_count\",\n",
    "    \"tls_common_name_count\",\n",
    "    \"tls_root_cert_validity_len\",\n",
    "    \"tls_leaf_cert_validity_len\",\n",
    "    \n",
    "    # new\n",
    "    \"tls_chain_len\",\n",
    "    \"tls_root_cert_lifetime\",\n",
    "    \"tls_leaf_cert_lifetime\",\n",
    "    \n",
    "    \n",
    "    # LEX ===============================\n",
    "    # old (Adam) & still used\n",
    "    \"lex_name_len\",\n",
    "    #\"lex_digit_count\", <-- abandoned, almost the same as \"lex_sub_digit_ratio\"\n",
    "    \"lex_has_digit\",\n",
    "    \"lex_phishing_keyword_count\",\n",
    "    \"lex_vowel_count\",\n",
    "    \"lex_underscore_hyphen_count\",\n",
    "    \"lex_consecutive_chars\",\n",
    "    \"lex_tld_len\",\n",
    "    \"lex_sld_len\",\n",
    "    \"lex_sub_count\",\n",
    "    \"lex_stld_unique_char_count\",\n",
    "    \"lex_begins_with_digit\",\n",
    "    \"lex_www_flag\",\n",
    "    \"lex_sub_max_consonant_len\",\n",
    "    \"lex_sub_norm_entropy\",\n",
    "    \"lex_sub_digit_count\",\n",
    "    \"lex_sub_digit_ratio\",\n",
    "    \"lex_sub_consonant_ratio\",\n",
    "    \"lex_sub_non_alphanum_ratio\",\n",
    "    \"lex_sub_hex_ratio\",\n",
    "    # new\n",
    "    \"lex_sld_norm_entropy\", # <-- newly added feature on 24-09-29\n",
    "    \n",
    "    # nothing\n",
    "    \n",
    "    # RDAP ===============================\n",
    "    # old (Adam) & still used\n",
    "    \"rdap_registration_period\",\n",
    "    \"rdap_has_dnssec\",\n",
    "    \n",
    "    # new\n",
    "    \"rdap_domain_age\",\n",
    "    \"rdap_time_from_last_change\",\n",
    "    \"rdap_domain_active_time\",\n",
    "    \n",
    "    # GEO ===============================\n",
    "    # old (Adam) & still used\n",
    "    \"geo_countries_count\",\n",
    "    \"geo_continent_hash\",\n",
    "    \"geo_countries_hash\"\n",
    "]\n",
    "\n",
    "print(\"Number of used features:\", len(used_features))\n",
    "\n",
    "df = df[[\"label\", *used_features]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) GPU and CUDA initialization\n",
    "- Normaly svm is not ideal for GPU, but feel free to play with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:  NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Using: \", torch.cuda.get_device_name(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  150260\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "########################################################\n",
    "# IMPORTANT: Dataset reduction for gridearch purposses #\n",
    "########################################################\n",
    "# For real training use fraction 1.0\n",
    "df_mini = df.sample(frac=0.4, random_state=1)\n",
    "\n",
    "# dump mini batch to csv\n",
    "#df_mini.to_csv('mini_batch.csv', index=False)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class_map = {\"benign_2307:unknown\": 0, \"misp_2307:phishing\": 1}\n",
    "\n",
    "labels = df_mini['label'].apply(lambda x: class_map[x]) # y vector\n",
    "features = df_mini.drop('label', axis=1).copy() # X matrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "features,\n",
    "labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "# fill nans with 0 in X_train and X_test and y_train and y_test\n",
    "    \n",
    "x_train = X_train.fillna(0)\n",
    "x_test = X_test.fillna(0)\n",
    "    \n",
    "y_train = y_train.fillna(0)\n",
    "y_test = y_test.fillna(0)\n",
    "    \n",
    "# convert x_train to numpy array\n",
    "x_train = x_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "    \n",
    "y_test = y_test.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "    \n",
    "    # Converting False and True to 0 and 1\n",
    "x_train = np.where(x_train == False, 0, x_train)\n",
    "x_train = np.where(x_train == True, 1, x_train)\n",
    "    \n",
    "x_test = np.where(x_test == False, 0, x_test)\n",
    "x_test = np.where(x_test == True, 1, x_test)\n",
    "    \n",
    "\n",
    "\n",
    "feature_count = x_train.shape[1]\n",
    "sample_count = x_train.shape[0]\n",
    "\n",
    "# print number of samples and features\n",
    "print(\"Number of samples: \", sample_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Training core\n",
    "1. Minmax data scale, (optimal would be do some categorical encoding...)\n",
    "2. Core SVM training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# MinMax data scaler\n",
    "scaler = MinMaxScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "# anotate function \n",
    "\n",
    "\n",
    "'''\n",
    "    @input kernel: string (rbf, linear, poly, sigmoid)\n",
    "    @input class_weight: dict or 'balanced'\n",
    "    @input C: float (default=1.0)\n",
    "    @input gamma: float (default='scale')\n",
    "    \n",
    "    @return accuracy_score: float\n",
    "    @return f1_score: float\n",
    "'''\n",
    "def fit_svm(kernel, class_weight, C, gamma):\n",
    "    svm = SVC(kernel=kernel, class_weight=class_weight, C=C, gamma=gamma, verbose=False)\n",
    "    svm.fit(x_train, y_train)\n",
    "    y_pred = svm.predict(x_test)\n",
    "\n",
    "    return accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search parameters\n",
    "\n",
    "To find optimal parameters for SVM, we use grid search.\n",
    "However, it is very time consuming, so we use only 10% of data for grid search. Or less. Even 1% is enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for rbf kernel, find optimal C and gamma using grid search and use f1 metric for scoring\n",
    "param_grid = {\n",
    "    'C': [50, 100, 150, 200, 300, 500, 1000],  # Centering around C=100\n",
    "    'gamma': [0.1, 0.5, 1, 2, 5],  # Centering around gamma=1\n",
    "    'kernel': ['rbf']  # Given 'rbf' gave the best result, let's focus on it\n",
    "}\n",
    "\n",
    "experimental_grid = {\n",
    "    'C': [0.1, 1, 10, 50, 100, 150, 200, 500, 1000, 2000, 5000],  # Wide range of C values\n",
    "    'gamma': [0.0001, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 'scale', 'auto'],  # Wide range of gamma values\n",
    "    'kernel': ['rbf']  # Given 'rbf' gave the best result, let's focus on it\n",
    "}\n",
    "\n",
    "optimal_grid = {\n",
    "    'C': [35, 40, 45, 50, 55, 60],  # Centering around C=100\n",
    "    'gamma': [0.8, 1, 1.2, 1.3, 1.4, 1.5],  # Centering around gamma=1\n",
    "    'kernel': ['rbf']  # Given 'rbf' gave the best result, let's focus on it\n",
    "}\n",
    "\n",
    "refined_grid = {\n",
    "    'C': [48, 49, 50, 51, 52, 53],\n",
    "    'gamma': [0.95, 0.96, 0.97, 0.98, 0.99, 1, 1.01, 1.02, 1.03, 1.04, 1.05],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "exploratory_grid = {\n",
    "    'C': [0.1, 1, 10, 50, 75, 100, 150, 200, 500, 1000, 2000, 5000],\n",
    "    'gamma': [0.0001, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 'scale', 'auto'],\n",
    "    'kernel': ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "coarse_grid = {\n",
    "    'C': [0.1, 1, 10, 100, 1000],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "# Use on your own risk and on very powerful machine\n",
    "giga_grid = {\n",
    "    'C': sorted(set([50, 100, 150, 200, 300, 500, 1000] + \n",
    "                    [0.1, 1, 10, 50, 100, 150, 200, 500, 1000, 2000, 5000] +\n",
    "                    [35, 40, 45, 50, 55, 60] +\n",
    "                    [48, 49, 50, 51, 52, 53] +\n",
    "                    [0.1, 1, 10, 50, 75, 100, 150, 200, 500, 1000, 2000, 5000] +\n",
    "                    [0.1, 1, 10, 100, 1000])),\n",
    "    'gamma': sorted(set([0.1, 0.5, 1, 2, 5] +\n",
    "                        [0.0001, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 'scale', 'auto'] +\n",
    "                        [0.8, 1, 1.2, 1.3, 1.4, 1.5] +\n",
    "                        [0.95, 0.96, 0.97, 0.98, 0.99, 1, 1.01, 1.02, 1.03, 1.04, 1.05] +\n",
    "                        [0.0001, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 'scale', 'auto'] +\n",
    "                        [0.001, 0.01, 0.1, 1, 10])),\n",
    "    'kernel': ['rbf', 'linear', 'poly', 'sigmoid']  # Including all kernel types as per exploratory_grid\n",
    "}\n",
    "\n",
    "\n",
    "# select GRID that you want to use, feel free to add your own\n",
    "# The default tested grid is optimal_grid\n",
    "param_grid = optimal_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of combinations:  36\n",
      "Expected time to complete grid search:  0.39  minutes\n",
      "F1 status: 0.6507936507936507 with accuracy: 0.9531914893617022 and params:  ['rbf', 35, 0.8]\n",
      "F1 status: 0.6559999999999999 with accuracy: 0.9542553191489361 and params:  ['rbf', 35, 1]\n",
      "F1 status: 0.6612903225806451 with accuracy: 0.9553191489361702 and params:  ['rbf', 40, 1]\n",
      "F1 status: 0.6612903225806451 with accuracy: 0.9553191489361702 and params:  ['rbf', 45, 1]\n",
      "F1 status: 0.6612903225806451 with accuracy: 0.9553191489361702 and params:  ['rbf', 50, 1]\n",
      "F1 status: 0.6612903225806451 with accuracy: 0.9553191489361702 and params:  ['rbf', 55, 1]\n",
      "DONE, highest precision:  0.6612903225806451 with accuracy: 0.95 and params:  ['rbf', 55, 1]\n"
     ]
    }
   ],
   "source": [
    "# Do estimation of time to complete grid search\n",
    "print(\"Number of combinations: \", len(param_grid['kernel']) * len(param_grid['C']) * len(param_grid['gamma']))\n",
    "\n",
    "\n",
    "# compute one iteration time\n",
    "\n",
    "start = time.time()\n",
    "fit_svm('rbf', 'balanced', 0.001, 0.0001)\n",
    "end = time.time()\n",
    "\n",
    "iteration_time = end - start\n",
    "\n",
    "# total time in minutes\n",
    "mins_total = (end - start) * len(param_grid['kernel']) * len(param_grid['C']) * len(param_grid['gamma']) / 60\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Expected time to complete grid search: \", round(mins_total, 2), \" minutes\")\n",
    "\n",
    "\n",
    "\n",
    "highest_f1 = 0\n",
    "highest_params = []\n",
    "\n",
    "params = []\n",
    "\n",
    "for kernel in param_grid['kernel']:\n",
    "    for C in param_grid['C']:\n",
    "        for gamma in param_grid['gamma']:\n",
    "            accuracy, f1 = fit_svm(kernel, 'balanced', C, gamma)\n",
    "            if f1 >= highest_f1:\n",
    "                highest_f1 = f1\n",
    "                highest_params = [kernel, C, gamma]\n",
    "                #save all params, f1 and accuracy to params list\n",
    "                params.append([kernel, C, gamma, f1, accuracy])\n",
    "                if(highest_f1 > 0):\n",
    "                    print(\"F1 status:\", highest_f1, \"with accuracy:\", accuracy, \"and params: \", highest_params)\n",
    "            print(\"=========================================\")\n",
    "            \n",
    "            \n",
    "print(\"DONE, highest precision: \", highest_f1, \"with accuracy:\", accuracy, \"and params: \", highest_params)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final SVM model training with best params that you will find using grid search\n",
    "\n",
    "print(fit_svm('rbf', 'balanced', 50, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes:\n",
      "Accuracy: 0.5574468085106383, F1 Score: 0.21509433962264152\n",
      "\n",
      "\n",
      "Logistic Regression:\n",
      "Accuracy: 0.9372340425531915, F1 Score: 0.28915662650602414\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "# Train Naive Bayes model and display results\n",
    "print(\"Naive Bayes:\")\n",
    "var_smoothing_option = float(input(\"Enter var_smoothing option (default is 1e-9): \") or 1e-9)\n",
    "gnb = GaussianNB(var_smoothing=var_smoothing_option)\n",
    "gnb.fit(x_train, y_train)\n",
    "y_pred = gnb.predict(x_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {acc}, F1 Score: {f1}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Train Logistic Regression model and display results\n",
    "print(\"Logistic Regression:\")\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(x_train, y_train)\n",
    "y_pred = logreg.predict(x_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {acc}, F1 Score: {f1}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
