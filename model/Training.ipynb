{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data loading and transformation\n",
    "_(You can skip to the next step if you already have the parquets in floor)_\n",
    "\n",
    "The data is stored in a MongoDB database. The `loader.py` script will load the data from the database, apply the transformers and save the result in an optimised way to a set of _parquet_ files. The transformers are defined in the `transformers` directory.\n",
    "\n",
    "The loader can be run from the command line, or we can import it as a module and use it in a notebook. It can either use cached data, which is stored in the `cache` directory once it has been loaded from the database, or it can load the data from the database directly. To load from the database, pass the `--cache-mode` or `-c` option to the CLI or use the `cache_mode` argument when running the loader's `run()` function. Possible values are `auto`, `force-local`, and `force-refresh`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from loader import run\n",
    "\n",
    "run_loader = False  # Set to True to trigger the loader. It won't do anything if the parquet files are already created.\n",
    "force_refresh = False  # Set to True to force grabbing and transformation of the latest data from DB.\n",
    "if run_loader:\n",
    "    mode = 'force-refresh' if force_refresh else 'auto'\n",
    "    run(cache_mode=mode)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model training\n",
    "\n",
    "The following cell configures the model training parameters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.IpcWriteOptions size changed, may indicate binary incompatibility. Expected 72 from C header, got 88 from PyObject\n"
     ]
    }
   ],
   "source": [
    "from model.data_loader import make_train_test, make_train, basic_preprocessor_df, basic_preprocessor_table\n",
    "import model.model_xgb as xgbm\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "model_name = \"my_lovely_model_1\"\n",
    "\n",
    "benign_parquet = \"../floor/benign_cesnet2_intersect\"\n",
    "malign_parquet = \"../floor/phishing\"\n",
    "benign_sample = 1.0\n",
    "malign_sample = 1.0\n",
    "\n",
    "use_gpu = False\n",
    "# If true, a search of optimal hyperparameters will be performed\n",
    "find_optimal_params = False\n",
    "\n",
    "# If true, the input data will be split into train/test subsets. If false, all the data will be used for training.\n",
    "split_train_test = False\n",
    "split_test_size = 0.3\n",
    "plot_model_evaluation = False\n",
    "\n",
    "# If true, cross-validation will be done for the specified model parameters, over the whole dataset\n",
    "do_cross_validation = False\n",
    "cross_validation_n_splits = 5\n",
    "\n",
    "# If find_optimal_params is true, this defines the number of splits for the internally used cross-validation\n",
    "optimal_params_search_n_splits = 5\n",
    "\n",
    "# Defines the model hyperparameters\n",
    "xgboost_params = {\n",
    "    \"max_depth\": 9,\n",
    "    \"eta\": 0.15,\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"min_child_weight\": 2.0,\n",
    "    \"subsample\": 0.6,\n",
    "    \"alpha\": 0,\n",
    "    \"gamma\": 0.1,\n",
    "    \"lambda\": 1.0,\n",
    "    \"max_delta_step\": 0,\n",
    "    \"grow_policy\": \"lossguide\",\n",
    "    \"max_bin\": 512,\n",
    "    \"tree_method\": \"gpu_hist\",\n",
    "    \"sampling_method\": \"gradient_based\"\n",
    "}\n",
    "xgboost_number_of_estimators = 290  # Number of \"trees\"\n",
    "\n",
    "optimal_params_search_grid = {\n",
    "    \"max_depth\": [9],\n",
    "    \"min_child_weight\": [2],\n",
    "    \"sampling_method\": [\"uniform\"],\n",
    "    \"subsample\": [0.6],\n",
    "    \"gamma\": [0.1],\n",
    "    \"grow_policy\": [\"lossguide\"],\n",
    "    \"max_bin\": [512],\n",
    "    \"n_estimators\": [280, 290, 300, 310],\n",
    "    \"lambda\": [1.0],\n",
    "    \"alpha\": [0.0]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "if split_train_test:\n",
    "    X_all, y_all, X_train, X_test, y_train, y_test, benign_label, malign_label = make_train_test(\n",
    "        benign_parquet, malign_parquet, basic_preprocessor_table, basic_preprocessor_df,\n",
    "        split_test_size, benign_sample, malign_sample)\n",
    "else:\n",
    "    X_all, y_all, benign_label, malign_label = make_train(benign_parquet, malign_parquet, basic_preprocessor_table,\n",
    "                                                          basic_preprocessor_df, benign_sample, malign_sample)\n",
    "    X_train, y_train = X_all, y_all"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Find the optimal set of hyperparameters\n",
    "\n",
    "scores = \"Hyperparameter search disabled\"\n",
    "if find_optimal_params:\n",
    "    scores = xgbm.find_optimal_model(X_train, y_train, optimal_params_search_grid, use_gpu,\n",
    "                                     optimal_params_search_n_splits)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "'Hyperparameter search disabled'"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Create the XGBoost model\n",
    "\n",
    "model = xgbm.make_model(xgboost_params, xgboost_number_of_estimators, use_gpu)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Do cross-validation\n",
    "\n",
    "if do_cross_validation:\n",
    "    xgbm.cross_validate_model(model, X_all, y_all, cross_validation_n_splits, 42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Train the model (and evaluate it â€“ only in split train/test mode)\n",
    "\n",
    "if split_train_test:\n",
    "    eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "    model.fit(X_train, y_train, eval_set=eval_set, verbose=False)\n",
    "else:\n",
    "    model.fit(X_train, y_train, verbose=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Plot evaluation results (only in split train/test mode)\n",
    "\n",
    "if split_train_test and plot_model_evaluation:\n",
    "    xgbm.plot_metrics(model.evals_result())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# Save the model\n",
    "\n",
    "if not os.path.exists(\"../stored_models\"):\n",
    "    os.makedirs(\"../stored_models\")\n",
    "\n",
    "with open(f\"../stored_models/{model_name}.model.pickle.dat\", \"wb\") as target_file:\n",
    "    pickle.dump(model, target_file)\n",
    "\n",
    "if split_train_test:\n",
    "    import pyarrow\n",
    "    import pyarrow.parquet\n",
    "\n",
    "    X_test[\"_labels\"] = y_test\n",
    "    # noinspection PyArgumentList\n",
    "    cache_table = pyarrow.Table.from_pandas(X_test)\n",
    "    pyarrow.parquet.write_table(cache_table, f\"../stored_models/{model_name}.test_data.parquet\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
